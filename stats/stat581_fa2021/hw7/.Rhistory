list(theta.dist=thetas,mu.est=mu.est,tau.est=tau.est,sigma.est=sigma.est)
}
mu.tau.gibbs(10000,xs)
mu.tau.gibbs(10000,xs)$theta.dist
plot(mu.tau.gibbs(10000,xs)$theta.dist)
plot(res$theta.dist[1000:1200,])
res <- mu.tau.gibbs(10000,x)
plot(res$theta.dist[1000:1200,])
x <- rnorm(200,mean=5,sd=2)
res <- mu.tau.gibbs(10000,x)
res$mu.est
res$tau.est
plot(res$theta.dist[1000:1200,])
plot(res$theta.dist[1000:1200,])
plot(res$theta.dist[1000:2000,])
plot(res$theta.dist[3000:4000,])
plot(res$theta.dist[5000:6000,])
plot(res$theta.dist[6000:6500,])
res
res[3]
1/res[3]
1./res[3]
sigma <- res$sigma.est
sigma
res$sigma.est
res[3]
res[4]
sigma <- res[4]
sigma
sigma
class(sigma)
sigma[1]
sigma[2]
sigma[1][1]
sigma[1][1][1]
as.numeric(sigma[1])
?dgamma
x <- read.table("coal.txt",header=T)[,2]
N <- length(x)
t <- function(theta) { sum(x[1:theta]) }
rlambda1 <- function(n,theta) { rgamma(n,t(theta)+1,theta+1) }
rlambda2 <- function(n,theta) { rgamma(n,t(N)-t(theta)+1,N-theta+1) }
ktheta <- Vectorize(function(theta,lambda1,lambda2)
{
(lambda1/lambda2)^t(theta)*exp(theta*(lambda2-lambda1))
},"theta")
ptheta <- function(theta,lambda1,lambda2)
{
Z <- sum(ktheta(1:N,lambda1,lambda2))
ktheta(theta,lambda1,lambda2)/Z
}
rtheta = function(n,lambda1,lambda2)
{
sample(x=1:N,n,replace=T,prob=ptheta(1:N,lambda1,lambda2))
}
ptheta(1)
ptheta(1,3,2)
sum(ptheta(1:N,3,2)
)
sum(ptheta(1:N,3,2))
N
sum(ptheta(1:N,3,2))
ptheta(1:N,3,2)
rtheta(100,3,2)
rtheta(1000,4,4)
warnings()
?floor
# thse compute the standardized weights
w = function(x) {
out <- q(x)/g(x)
out/sum(out)
}
h <- function(x) { x^2 }
ry <- function(n)
{
ys <- numeric(n)
ws <- rexp(n=n,rate=1)
us <- runif(n)
for (i in 1:n) { ys[i] <- ifelse(us[i] < 0.5,-ws[i],ws[i]) }
ys
}
q <- function(x) { exp(x)/(exp(2*x)+1) }
g <- function(x) { exp(-abs(x)) }
n=100
m=500
ys=ry(m)
ys
ws=w(ys)
ws
xs <- sample(ys,n,replace=T,prob=ws)
xs
w(xs)
w(xs)
xs <- sample(ys,n,replace=T,prob=ws)
c(1,2,3)*c(1,2,3)
mean(c(1,2,3)*c(1,2,3))
ys <- ry(m)
ws <- w(ys)
ws
ws*ys
ys
ws*ys
ys
mean(ws*ys)
m <- n*floor(sqrt(n))
ys <- ry(m)
ws <- w(ys)
mean(w(xs)*h(xs))
ys
ys^2
rx.transform <- function(n)
{
us <- runif(n)
log(tan(pi/2*us))
}
x <- rx.transform(10000)
print(mean(x^2))
xss <- seq(0.01,10.0,by=.01))
xss <- seq(0.01,10.0,by=.01)
yss <- g(seq(0.01,10.0,by=.01))
max(yss)
yss <- g(seq(0.001,10.0,by=.001))
max(yss)
yss <- g(seq(0.0001,10.0,by=.0001))
max(yss)
sum(ws*ys)
rx.importance(1000)
ls
?sample
?matrix
rtheta.prior <- function(n)
{
samp <- matrix(nrow=n,ncol=3)
samp[,1] <- rgamma(n,shape=3,rate=1)
samp[,2] <- rgamma(n,shape=3,rate=1)
samp[,3] <- sample(n:(N-1),1,replace=T)
samp
}
gibbs <- function(n,burn=1000,prior=rtheta.prior)
{
t <- function(theta) { sum(x[1:theta]) }
rlambda1 <- function(theta) { rgamma(t(theta)+1,theta+1) }
rlambda2 <- function(theta) { rgamma(t(N)-t(theta)+1,N-theta+1) }
ktheta <- Vectorize(function(theta,lambda1,lambda2)
{
if (theta < 1 || theta >= N) { return(0) }
(lambda1/lambda2)^t(theta)*exp(theta*(lambda2-lambda1))
},"theta")
ptheta <- function(theta,lambda1,lambda2)
{
Z <- sum(ktheta(1:N,lambda1,lambda2))
ktheta(theta,lambda1,lambda2)/Z
}
rtheta = function(lambda1,lambda2)
{
sample(x=1:N,n,replace=T,prob=ptheta(1:N,lambda1,lambda2))
}
nn <- n+burn
thetas <- matrix(nrow=nn,ncol=3)
thetas[1,] <- prior(1)
print(thetas[1,])
#for (i in 1:(nn-1))
#{
#  theta.new <- rtheta(thetas[i,2],thetas[i,3])
#  lambda1.new <- rlambda1(theta.new)
#  lambda2.new <- rlambda2(theta.new)
#  thetas[i+1,] <- c(theta.new,lambda1.new,lambda2.new)
#}
#thetas <- thetas[(burn+1):nn,]
#theta.est <- mean(thetas[,1])
#lambda1.est <- mean(thetas[,2])
#lambda2.est <- mean(thetas[,3])
#list(theta.dist=thetas,
#     theta.est=theta.est,
#     lambda1.est=lambda1.est,
#     lambda2.est=lambda2.est)
}
gibbs(10)
x <- read.table("coal.txt",header=T)[,2]
N <- length(x)
x
N
class(x)
type(x)
x
numeric(10)
class(numeric(10))
ktheta <- Vectorize(function(theta)
{
if (theta < 1 || theta >= N) { return(0) }
(lambda1/lambda2)^t(theta)*exp(theta*(lambda2-lambda1))
})
lambda1 <- 2
lambda2 <- 3
N <- 112
ktheta(1)
ktheta(111)
ktheta(112)
ktheta(113)
ktheta(1:N)
sum(ktheta(1:N))
ktheta(1:N)
sum(ktheta(1:N))
sample(x=1:(N-1),n,replace=T,prob=ktheta(1:(N-1)))
sample(x=1:N,n,replace=T,prob=ktheta(1:N))
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/2)
sum(ktheta(1:N))
Z <- sum(ktheta(1:N))
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
ktheta(1:N)/Z
sum(ktheta(1:N)/Z)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z/10)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z/10000)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
lambda1
lambda1=3
lambda2=.925
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
ktheta(1:N)/Z
Z <- sum(ktheta(1:N))
Z
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
sample(x=1:N,n,replace=T,prob=ktheta(1:N)/Z)
prob=ktheta(1:N)/Z
prob
cumsum(prob)
install.packages("ISLR2")
?ISLR2
?islr2
remotes::install_github("rstudio/blogdown")
ls
load(.Rdata)
load(".Rdata")
blowdown::serve_site()
blogdown::serve_site()
ls
blogdown::serve_site()
blogdown::serve_site()
getwd()
?head
wireless.data <- read.csv("https://goo.gl/72BKSf", header = TRUE)
str(wireless.data)
library(mosaic)
rbinom(n = 1, size = 20, prob = 0.6)
rbinom(n = 10, size = 20, prob = 0.6)
sum(rbinom(n = 100, size = 20, prob = 0.6))/100
sum(rbinom(n = 1000, size = 20, prob = 0.6))/1000
sum(rbinom(n = 1000, size = 20, prob = 0.6))/1000
sum(rbinom(n = 10000, size = 20, prob = 0.6))/10000
(x <- rbinom(n = 50, size = 20, prob = 0.6))
histogram(x)
(x <- rbinom(n = 5000, size = 20, prob = 0.6))
histogram(x)
x <- rbinom(n = 100000, size = 20, prob = 0.6)
histogram(x)
H1000 <- rbinom(n = 1000, size = 40, prob = 0.357)
tally(H1000)
favstats(H1000)
H1000 <- rbinom(n = 1000, size = 40, prob = 0.357)
dbinom(x = 15, size = 40, prob = 0.357)
dbinom(x = 0:20, size = 40, prob = 0.357)
plot(dbinom(x = 0:20, size = 40, prob = 0.357))
xpnorm(q = 48, mean = 46.8, sd = 1.75)
xpbinom(.5,100,.95)
xpbinom(.5,10,.95)
xpbinom(.5,100)
xpbinom(.5,100,.5)
?xpbinom(.5,100,.5)
xpnorm( c(47, 49), mean = 46.8, sd = 1.75)
xpnorm(c(47, 49), mean = 46.8, sd = 1.75)
xpnorm(c(47, 49), mean = 46.8, sd = 1.75)
knitr::opts_chunk$set(echo = TRUE)
n = 40
p = .75
plotDist('binom',size=n, p=p)
plotDist('binom',size=n, prob=p)
dbinom(30,n,p)
?plotDist
?rnorm
?xtabs
?contr.sum
?par
library("readxl")
h5.data = read_excel("handout5data.xlsx")
str(h5.data)
#We will also be performing multiple comparisons
library("multcomp")
P = as.factor(na.omit(h5.data$pressure))
T = as.factor(na.omit(h5.data$temperature))
yield = na.omit(h5.data$yield)
h5.data = read_excel("handout5data.xlsx")
setwd("classes/stat589_fa2021/hw/hw5/")
h5.data = read_excel("handout5data.xlsx")
P = as.factor(na.omit(h5.data$pressure))
T = as.factor(na.omit(h5.data$temperature))
yield = na.omit(h5.data$yield)
x.mod = aov(yield ~ P*T)
summary(x.mod)
a.mod = aov(yield ~ P+T)
summary(a.mod)
a.means = predict(a.mod)
a.means
interaction.plot(T,P,yield)
interaction.plot(T,P,a.means)
interaction.plot(T,P,yield)
interaction.plot(T,P,a.means)
compare.P = glht(a.mod, linfct = mcp( T = "Tukey"))
compare.T = glht(a.mod, linfct = mcp( T = "Tukey"))
compare.T
cld(summary(compare.T,test=univariate()))
plot(cld(summary(compare.T,test=univariate())))
confint(compare.T,calpha=univariate_calpha())
#We begin by calling the data for handout 7
library("readxl")
h7.data = read_excel("handout7data.xlsx")
?t.test
?Pair
Pair(c(1,2,3),c(10,20,30))
#We will need these packages to work with blocks as random effects
library("lme4")
library("lmerTest")
#We will need this package for performing Fisher comparisons and groupings
library("multcomp")
#We will need these packages to work with blocks as random effects
library("lme4")
library("lmerTest")
#We will need this package for performing Fisher comparisons and groupings
library("multcomp")
#Example 7.1
#An experiment is conducted to compare two tips used on a hardness testing machine
#The experimental units are the metal specimens. Each tip gives a hardness measurement for each specimen.
#Thus, the data is from a paired comparisons design
tip1 = na.omit(h7.data$tip1)
tip2 = na.omit(h7.data$tip2)
tip1
tip2
#The built-in function t.test can be used to compute a paired comparisons analysis.
t.test(tip1,tip2,paired = TRUE)
#The built-in function t.test can be used to compute a paired comparisons analysis.
t.test(tip1,tip2,paired = TRUE)
library("readxl")
data = read_excel("handout7data.xlsx")
A = as.factor(na.omit(data$o))
B = as.factor(na.omit(data$fuse))
y = na.omit(data$time)
A
B
A1 = A[B==1]
B
B = B[A==1]
A
A = as.factor(na.omit(data$o))
B = as.factor(na.omit(data$fuse))
y = na.omit(data$time)
B1 = B[A==1]
B2 = B[A==2]
B1
B2
B
t = data$t
s = data$s
h = data&h
# use contrasts to define parameter restrictions for the fixed effect in the model
contrasts(t)=contr.sum
h = data$h
# use contrasts to define parameter restrictions for the fixed effect in the model
contrasts(t)=contr.sum
t = as.factor(data$t)
s = as.factor(data$s)
h = data$h
# use contrasts to define parameter restrictions for the fixed effect in the model
contrasts(t)=contr.sum
random.mod = lmer(h ~ (1|s) + t)
#The anova command is used to compute the test for fixed effects.
anova(random.mod)
random.mod
summary(random.mod)
# the 1 in front of s signifies that batch levels are randomly selected from a
# common distribution.
random.mod = lmer(h ~ (1|s) + t)
anova(random.mod)
coeff(random.mod)
coef(random.mod)
tip = as.factor(data$tip) # tip level factor : fixed effect
spec = as.factor(data$specimen) # metal specimen : random effect
hard = data$hardness
# fixed effect tau1 + tau2 = 0 (for tip1 and tip2)
contrasts(tip)=contr.sum
# the 1 in front of s signifies that batch levels are randomly selected from a
# common distribution.
random.mod = lmer(hard ~ (1|spec) + tip)
#The anova command is used to compute the test for fixed effects.
anova(random.mod)
round(.00087,digits=3)
anova(random.mod)
random.mod
coef(random.mod)
anova(random.mod)
summary(random.mod)
summary(random.mod)$`Random effects`
x=summary(random.mod)
x$methTitle
x$devcomp
x$sigma
x$vcov
x$ngrps
x$family
x$logLik
x$coefficients
x$varcor
x$varcor^2
sss=x$varcor
sss$spec
sss^2
sss$spec^2
sss$spec
sss$spec$`Intercept`
x$varcor
x$AICtab
x$objClass
coef(x)
cov(x)
vcov(x)
sigma(x)
var(x)
x$isLmer
x$devcomp
summary(random.mod)
# since the fixed effect estimates must sum to 0, the estimate at level a=4 is
# the negative of the sum of the fixed effect estimates at levels 1 through a-1.
estimates = summary(random.mod)
estimates.tau.hat = c(estimates$coefficients[1:4,1],0-sum(estimates$coefficients[2:4,1]))
estimates.tau.hat
1:4
as.string(1:4)
as.str(1:4)
toString(1:4)
names(estimates.tau.hat) = "tau" + c("1","2","3","4")
"tau" + c("1","2","3","4")
c("1","2","3","4")
concat("a",c("1","2","3","4"))
cat("a",c("1","2","3","4"))
paste("a",c("1","2","3","4"))
names(estimates.tau.hat) = paste("tau",c("1","2","3","4"))
estimates.tau.hat
estimates.tau.hat = c(estimates$coefficients[2:4,1],0-sum(estimates$coefficients[2:4,1]))
names(estimates.tau.hat) = paste("tau",c("1","2","3","4"))
estimates.tau.hat
estimates.tau.hat = c(estimates$coefficients[1:4,1],0-sum(estimates$coefficients[2:4,1]))
names(estimates.tau.hat) = c("mu","tau1","tau2","tau3","tau4")
estimates.tau.hat
?t
?qt
qt(.025,9)
qt(1-.025,9)
random.mod
.25793^2
?random.mod
random.mod
xxxx=random.mod
coef(random.mod)
vcov.merMod(random.mod)
estimates.tau.hat
summary(random.mod)$`Random effects`
anova(random.mod)
random.mod = lmer(h ~ (1|s) + t)
?lmer
sigma(random.mod)
# the 1 in front of s signifies that batch levels are randomly selected from a
# common distribution.
random.mod = lmer(hard ~ (1|spec) + tip)
sigma(random.mod)
tip = as.factor(data$tip) # tip level factor : fixed effect
spec = as.factor(data$specimen) # metal specimen : random effect
hard = data$hardness
# fixed effect tau1 + tau2 = 0 (for tip1 and tip2)
contrasts(tip)=contr.sum
# the 1 in front of s signifies that batch levels are randomly selected from a
# common distribution.
random.mod = lmer(hard ~ (1|spec) + tip)
#The anova command is used to compute the test for fixed effects.
anova(random.mod)
random.mod
summary(random.mod)
summ=summary(random.mod)
summ$devcomp
summ$sigma
summ$residuals
summ$coefficients
summ$varcor
summ$varcor$spec
.257930^2
.094281^2
varcor(summ)
varcor(summ)
.094281^2
(.0089)^2
(.094281)^2
(.094)^2
.257^2
.258^2
aov(y~A+B)
anova(aov(y~A+B))
?interaction.plot
interaction.plot(A,B,y)
interaction.plot(B,A,y)
curwd()
