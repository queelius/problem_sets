---
title: 'Time Series Analysis - 478 - Exam 2'
author: "Alex Towell (atowell@siue.edu)"
header-includes:
 - \usepackage{amsmath}
 - \usepackage{mathtools}
 - \usepackage{amsthm}
 # - \usepackage{amsbsy}
 - \usepackage{bm}
output:
  pdf_document:
    latex_engine: xelatex
    df_print: kable
    toc: true
    toc_depth: 3
  html_document:
    df_print: paged
---

\newcommand{\backshift}{\operatorname{B}}
\newcommand{\var}{\operatorname{Var}}
\newcommand{\expect}{\operatorname{E}}
\newcommand{\corr}{\operatorname{Corr}}
\newcommand{\cov}{\operatorname{Cov}}
\newcommand{\ssr}{\operatorname{SSR}}
\newcommand{\se}{\operatorname{SE}}
<!-- \newcommand{\mat}[1]{\pmb{#1}}  -->
\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\eval}[2]{\left. #1 \right\vert_{#2}}

# Problem 1.1
Suppose that simple exponential smoothing is being used to forecast the process
$y_t = \mu + e_t$, where where $\{e_t\}$ are white noise with mean $0$ and
variance $\sigma^2$.
At the start of period $t^{∗}$, the mean of the process
experiences a transient;
that is, it shifts to a new level $\mu + \delta$, but
reverts to its original level $\mu$ at the start of the next period $t^{∗} + 1$.
The mean remains at this level for subsequent time periods.

## Part (a)
\fbox{\begin{minipage}{.8\textwidth}
Find the expected value of the simple exponential smoother
$$
   \tilde{y}_T = (1-\theta)\sum_{t=0}^{\infty} \theta^t y_{T}.
$$
\end{minipage}}

We have a time series
$$
   y_t = \mu + e_t
$$
except at $y_{t^*}$ which is distributed
$$
   y_t^{*} = \mu + \delta + e_{t^*}
$$
where the error terms are zero mean white noise with variance $\sigma^2$.

The expectation of the smoothed time series $\tilde{y}_T$ is given by
\begin{align*}
   \expect(\tilde{y}_T)
      &= (1-\theta)\sum_{t=0}^{\infty} \theta^t \expect(y_{T-t})\\
      &= (1-\theta)\left(
            \sum_{t=0}^{T-t^*+1} \theta^t \mu +
            \theta^{T-t^*}(\mu + \delta) +
            \sum_{t=T-t^*-1}^{\infty} \theta^t \mu
         \right)\\
      &= (1-\theta)\left(
            \sum_{t=0}^{\infty} \theta^t \mu +
            \theta^{T-t^*} \delta
         \right)\\
      &= \mu + (1-\theta) \theta^{T-t^*} \delta.
\end{align*}

## Part (b)
\fbox{\begin{minipage}{.8\textwidth}
For $\theta = 0.5$, determine the number of periods that it will take following
the impulse for the expected value of $\tilde{y}_T$ to return to within $0.1\delta$ of the original level $\mu$.
\end{minipage}}
We wish to find $\tilde{y}_k$ such that it is expected to be within
$\frac{1}{10} \delta$ of $\mu$,
$$
   \left\lvert \expect(\tilde{y}_k) - \mu \right\rvert \leq \left\lvert \frac{1}{10} \delta \right\rvert.
$$

Plugging in the definition of the expectation and simplifying,
$$
   \left\lvert (1-\theta) \theta^{k-t^*} \delta \right\rvert \leq \left\lvert \frac{1}{10} \delta \right\rvert.
$$

Since pulling all positive numbers (or symbols that stand for positive numbers) out of the
absolute value function does not change the expression, we may rewrite the above
as
$$
   (1-\theta) \theta^{k-t^*} \vert \delta \vert \leq \frac{1}{10} \vert \delta \vert.
$$
Dividing by $\vert \delta \vert$ on both sides,
$$
   (1-\theta) \theta^{k-t^*} \leq \frac{1}{10},
$$
which may be rewritten as
$$
   \theta^k \leq \frac{\theta^{t^*}}{10(1-\theta)}.
$$

Taking the logarithm of both sides
\begin{align*}
   k \log \theta
      &\leq \log \left(\frac{\theta^{t^*}}{10(1-\theta)}\right)\\
      &\leq t^* \log \theta - \log 10 - \log(1-\theta).
\end{align*}

Finally, we isolate $k$ by dividing by $\log \theta$ on both sides. However,
note that $\log \theta$ is negative, and so we must flip the inequality,
$$
   k \geq t^* - \frac{\log 10}{\log\theta} - \frac{\log(1-\theta)}{\log \theta}.
$$

Letting $\theta = 0.5$,
$$
   k \geq t^* + \frac{\log 10}{\log 2} - \frac{\log 0.5}{\log 0.5}
$$
which simplifies to
$$
   k \geq t^* + 2.32.
$$

We wish to take the \emph{smallest} $k$ that is an integer that satisfies the
equation. Thus, $k = t^* + 3$. Or, in other words, $3$ periods after $t^*$,
$\tilde{y}_T$ has an expectation that is within the specified distance of $\mu$.

# Problem 1.2
Let $\{Y_t\}$ be an AR(1) process with $|φ| < 1$. That is $Y_t = φY_{t−1} + e_t$,
where $\{e_t\}$ are white noise with mean $0$ and variance $\sigma^2$.
Also note $e_t$’s are independent of $Y_{t−1}, Y_{t−2},\ldots$.

## Part (a)
\fbox{\begin{minipage}{.8\textwidth}
Find the autocorrelation function for $W_t = Y_t - Y_{t-1}$ in terms of $φ$ and $\sigma^2$.
\end{minipage}}

Observe that
$$
   W_t = Y_t - Y_{t-1} = φ Y_{t-1} + e_t - Y_{t-1}
$$
and thus
$$
   W_t = (φ - 1) Y_{t-1} + e_t.
$$

The autocovariance function for $W_t$, denoted by $γ_{\{W_t\}}$, is defined as
$$
   γ_{\{W_t\}}(k) = \cov(W_t,W_{t-k}).
$$

Assuming $k \neq 0$ (we solve directly for variance in that case) and replacing
$W_t$ and $W_{t-k}$ with their respective definitions yields
\begin{align*}
   γ_{\{W_t\}}(k)
      &= \cov((φ - 1) Y_{t-1} + e_t,(φ - 1) Y_{t-k-1} + e_{t-k})\\
      &= \cov((φ - 1) Y_{t-1},(φ - 1) Y_{t-k-1})\\
      &= (φ - 1)^2 \cov(Y_{t-1},Y_{t-k-1}).
\end{align*}

Observe that $\cov(Y_{t-1},Y_{t-k-1}) = γ_{\{Y_t\}}(k)$.
Since $\{Y_t\}$ is $\operatorname{AR}(1)$,
$$
   γ_{\{Y_t\}}(k) = \sigma^2 \frac{φ^k}{1-φ^2}.
$$
Thus,
$$
   γ_{\{W_t\}}(k) = γ_{\{Y_t\}}(k) = \sigma^2 \frac{φ^k}{1-φ^2}.
$$

The variance of $\{W_t\}$ is given by
\begin{align*}
   \cov(W_t,W_t)
      &= \cov((φ - 1) Y_{t-1} + e_t, (φ - 1) Y_{t-1} + e_t)\\
      &= (φ - 1)^2 \cov(Y_{t-1},Y_{t-1}) + \cov(e_t,e_t)\\
      &= (φ - 1)^2 \frac{\sigma^2}{1-φ^2}  + \sigma^2\\
      &= \sigma^2\left(1 + \frac{(φ - 1)^2}{1-φ^2}\right).
\end{align*}

Thus, the autocorrelation function is given by
$$
   \rho_k = \frac{γ_{\{W_t\}}(k)}{γ_{\{W_t\}}(0)} = \frac{\sigma^2 \frac{φ^k}{1-φ^2}}{\sigma^2\left(1 + \frac{(φ - 1)^2}{1-φ^2}\right)},
$$
which simplifies to
$$
   \rho_k = \frac{\frac{φ^k}{1-φ^2}}{1 + \frac{(φ - 1)^2}{1-φ^2}} = \frac{φ^k}{2(1-φ)}.
$$

## Part (b)
In part (a), we found that
$$
   \var(W_t) = \sigma^2\left(1 + \frac{(φ - 1)^2}{1-φ^2}\right).
$$

# Problem 1.3
Suppose $Y_t = X_t + e_t$, where $\{e_t\}$ are normal white noise with mean $0$ and
variance $\sigma_e^2$.
The $\{X_t\}$ process is a stationary AR(1) defined by $X_t = φX_{t−1} + Z_t$,
where $\{Z_t\}$ is a zero mean normal white noise process with variance $\sigma_Z^2$.
As usual, in the AR(1) process, assume that $Z_t$ is independent
of $X_{t−1}, X_{t−2}, \ldots$.
Assume additionally that $\expect(e_t Z_s) = 0$ for all $t$ and $s$.

## Part (a)
\fbox{Show that $\{Y_t\}$ is stationary and find its autocovariance function, $γ_k$.}

To be stationary, $\{Y_t\}$ must have a constant mean and a a autocovariance that
is strictly a function of the lag.

The mean is given by
$$
   \expect(Y_t) = \expect(X_t)  + \expect(e_t).
$$ 
Since $X_t$ is AR(1) with mean $\delta / (1-φ) = 0$, we see that $\expect(Y_t) = 0$,
i.e., is a constant zero.

The variance is given by
$$
   \var(Y_t) = \var(X_t)  + \sigma^2.
$$
Since $X_t$ is AR(1), its variance is $\sigma_Z^2/(1-φ^2)$, thus
$$
   \var(Y_t) = \sigma_Z^2/(1-φ^2) + \sigma^2.
$$

The autocovariance of $\{Y_t\}$ is given by
$$
   γ_k = \cov(Y_t,Y_{t-k}) = \expect(Y_t Y_{t-k}) - \expect(Y_t)\expect(Y_{t-k}).
$$
Since $\{Y_t\}$ has a constant expectation of zero, this simplies to
$$
   γ_k = \cov(Y_t,Y_{t-k}) = \expect(Y_t Y_{t-k}).
$$

Observe that $Y_t = φ X_{t-1} + Z_t + e_t$ and
$$
   Y_t Y_{t-k} = (φ X_{t-1} + Z_t + e_t) Y_{t-k} = φ Y_{t-k} X_{t-1} + Y_{t-k} Z_t + Y_{t-k} e_t.
$$

The expectation of $Y_t Y_{t-k}$ is given by
\begin{align*}
   \expect(Y_t Y_{t-k})
      &= φ \expect(Y_{t-k} X_{t-1}) + \expect(Y_{t-k} Z_t) + \expect(Y_{t-k} e_t)\\
      &= φ \expect(Y_{t-k} X_{t-1}) + \expect(Y_{t-k}) \expect(Z_t) + \expect(Y_{t-k}) \expect(e_t)\\
      &= φ \expect(Y_{t-k} X_{t-1})\\
      &= φ \expect((X_{t-k} + e_t) X_{t-1})\\
      &= φ \expect(X_{t-1} X_{t-k} + e_t X_{t-1})\\
      &= φ \left(\expect(X_{t-1} X_{t-k}) + \expect(e_t X_{t-1})\right)\\
      &= φ \expect(X_{t-1} X_{t-k}).
\end{align*}

Since $\{X_t\}$ is AR(1), observe that the autocovariance function for $\{X_t\}$ is
$γ_{\{X_t\}}(k) = φ \expect(X_{t-1} X_{t-k})$, which has a closed-form solution
\begin{equation}
   γ_{\{X_t\}}(k) =
   \begin{cases}
      \frac{\sigma_Z^2}{1-φ^2}   & k = 0\\
      φ γ_{\{X_t\}}(k-1)         & k > 0.
   \end{cases}
\end{equation}

Thus, the autocovariance function for $\{Y_t\}$ is given by
\begin{equation}
   γ_k =
   \begin{cases}
      \frac{\sigma_Z^2}{1-φ^2} + \sigma_e^2   & k = 0\\
      γ_{\{X_t\}}(k)         & k > 0.
   \end{cases}
\end{equation}

Since its autocovariance function is strictly a function of lag and its
mean is a constant zero, $\{Y_t\}$ is stationary. Note that it is not just
weakly stationary, but strongly stationary given the normally distributed
random errors.

## Part (b)
\fbox{\begin{minipage}{.8\textwidth}
Show that the process $\{U_t\}$, where $U_t = Y_t − φY_{t−1} = (1 − φB)Y_t$,
has nonzero correlation only at lag 1 (excluding lag 0, of course!).
\end{minipage}}

The autocovariance is given by
\begin{align*}
   γ_{\{U_t\}}(k)
      &= \cov(U_t,U_{t-k})\\
      &= \cov(Y_t - φ Y_{t-1},Y_{t-k} - φ Y_{t-k-1}).
\end{align*}

Observe that $Y_t - φ Y_{t-1} = X_t + e_t - φ(X_{t-1} + e_{t-1})$.
Since $Z_t = X_t - φ X_{t-1}$, we see that
$$
   Y_t - φ Y_{t-1} = e_t + Z_t - φ e_{t-1}
$$
and
$$
   Y_{t-k} - φ Y_{t-k-1} = e_{t-k} + Z_{t-k} - φ e_{t-k-1}.
$$

Thus,
$$
   γ_{\{X_t\}}(k) = \cov(e_t + Z_t - φ e_{t-1}, e_{t-k} + Z_{t-k} - φ e_{t-k-1}).
$$
If $k > 1$, then $γ_{\{X_t\}}(k) = \cov(e_t + Z_t - φ e_{t-1}, e_{t-k} + Z_{t-k} - φ e_{t-k-1}) = 0$
since they have no terms in common.
If $k=1$, then
\begin{align*}
   γ_{\{X_t\}}(1)
      &= \cov(e_t + Z_t - φ e_{t-1}, e_{t-1} + Z_{t-1} - φ e_{t-2})\\
      &= \cov(-φ e_{t-1}, e_{t-1})\\
      &= -φ \var(e_{t-1})\\
      &= -φ \sigma_e^2,
\end{align*}
which is the only lag that is non-zero.


# Problem 1.4
Suppose that $\{e_t\}$ is a zero mean white noise process with variance $\sigma^2$.
Consider:
\begin{enumerate}
\item[(i)] $y_t = 0.80y_{t−1} − 0.15y_{t−2} + e_t − 0.30e_{t−1}$
\item[(ii)] $y_t = y_{t−1} − 0.50y_{t−2} + e_t − 1.2e_{t−1}$.
\end{enumerate}

## Part (a)
\fbox{Identify each model as an ARMA(p, q) process; that is, specify $p$ and $q$.}

\begin{enumerate}
\item We rewrite equation (i),
$$
   y_t = 0.80 \backshift y_t − 0.15 \backshift^2 y_t + e_t − 0.30 \backshift e_t.
$$       
Now, we rewrite it into the form
\begin{align*}
   (1 - 0.8 B + 0.15 B^2) y_t &= (1 - 0.3 B) e_t\\
   -20 (1 - 0.5 B)(1 - 0.3 B) y_t &= (1 - 0.3 B) e_t\\
   -20 (1 - 0.5 B) y_t &= e_t.
\end{align*}

We see that $y_t = 0.5 y_{t-1} - \frac{e_t}{20}$.
Two things should be pointed out.
First, assuming $e_t$ is symmetric with zero mean,
$- \frac{e_t}{20}$ is distributed the same as $\frac{e_t}{20}$.
Next, the variance of $\frac{e_t}{20}$ is $\frac{1}{400} \sigma^2$.

We let $W_t = \frac{1}{20} e_t$, and thus
$$
   y_t = 0.5 y_{t-1} + W_t,
$$
where $\{W_t\}$ is a zero mean white noise process with variance $\frac{1}{400} \sigma^2$
and $\{y_t\}$ is AR(1).

\item We rewrite equation (ii),
$$
   y_t = B y_t - 0.5 B^2 y_t + e_t - 1.2 B e_t.
$$       
Now, we rewrite it into the form
\begin{align*}
   (1 - B + 0.5 B^2) y_t &= (1 - 1.2 B) e_t\\
   0.5 (B - 1 + i)(B - 1 - i) y_t &= (1 - 1.2 B) e_t.
\end{align*}
We see that this is an ARMA(2,1) process.
\end{enumerate}

## Part (b)
\fbox{Determine whether each model is stationary and/or invertible.}
Time series (i) is AR(1) and is thus invertible.
We also know that it is stationary since $|φ| = |0.5| < 1$.

Time series (ii) is ARMA(2,1).
Let $φ(x) = (x - 1 + i)(x - 1 - i)$ which has roots $1+i$ and $1-i$, which both
modulus $\sqrt{2}$. This is larger than $1$, so it is invertible.
Let $\theta(x) = 1 - 1.2 x$ which has root $0.8\overbar{3}$. Since $|0.8\overbar{3}| < 1$,
it is not stationary.


















# Problem 2.1
The Johnson and Johnson dataset contains quarterly earnings per share for the
U.S. company Johnson & Johnson.
There are 84 quarters (21 years) measured from the first quarter of 1960 to the
last quarter of 1980.
To load the dataset, run the following: install.packages(”astsa”); library(astsa).
The dataset is under the name jj.
Do a log transformation of the original time series before answering the following.

## Preliminary analysis
We would like to take a look at a simple plot of the data, prior to any
transformations.
```{r}
library(astsa)
tsdata <- ts(data=jj)
plot(tsdata)
```

We see that the variance increases over time.
The log-transformation will fix this problem, as computed in the following code:
```{r}
n <- length(tsdata)
A <- exp((1/n)*sum(log(tsdata)))
ys <- A*log(tsdata)
log_j <- log(tsdata)
```

## Part (a)
\fbox{\begin{minipage}{.8\textwidth}
Construct a time series plot for the logged data.
Comment on overall trend and seasonality variation.
\end{minipage}}

We generate the plot with the following R code:
```{r}
plot(ys)
```

The data has both seasonality and a (positive) trend.

## Part (b)
\fbox{\begin{minipage}{.8\textwidth}
Fit the a regression model on the logged data
$$
   y_t = β_0 + β_1 t + α_1 Q_2(t) + α_2 Q_3(t) + α_3 Q_4(t) + e_t,
$$
where $Q_i(t) = 1$ if time $t$ corresponds to quarter $i = 1, 2, 3$ and zero
otherwise.
Assume $e_t$ is a normal white noise sequence.
Report model coefficients estimates.
Superimpose the fitted values on the time plot in part (a).
Note: you will need to first create a variable for time and quarter.
To do that, you may use: t=1:84; qt=as.factor(rep(1:4,21)).
\end{minipage}}

We perform the model fitting using the following R code:
```{r}
t <- 1:n
qt <- as.factor(rep(1:4,(n/4)))
q1 <- qt==1
q2 <- qt==2
q3 <- qt==3
m <- cbind(t,q1,q2,q3,ys)

# fit regression model to data
fit <- lm(ys~t+q1+q2+q3, data=m)
fit2 <- lm(log_j~t+q1+q2+q3, data=m)



qt2 <- as.factor(rep(1:4,(n/4)))
fit3 <- lm(log_j~t+qt)

# better approach:
#    fit <- lm(ys~t+qt)
# where qt are the factors (1,2,3,4)
```

The model coefficients are given by:
```{r}
summary(fit)
summary(fit2)
summary(fit3)
```

In other words, the estimate is given by
$$
   \hat{y}_t = -2.508 + 0.126 t + 0.514 Q_1(t) + 0.599 Q_2(t) + 0.811 Q_3(t).
$$

The plot of the data with $\hat{y}_t$ superimosed onto it is given by:
```{r}
library(latex2exp)
plot(ys,col="blue", pch=19,xlab=TeX("quarter ($t$)"),ylab=TeX("$y_t$"),, main="Log-transformed fit")
lines(fitted.values(fit),type="l")
legend(1,8,legend=c("data","model"),col=c("blue","black"),lty=1:2,cex=0.8)
```

We use the following R code to show the model fit with the log-transformation reverse.
```{r}
library(latex2exp)
plot(tsdata,col="blue", pch=19,xlab=TeX("quarter ($t$)"),ylab=TeX("$jj_t$"), main="Untransformed fit")
lines(exp(fit$fitted.values/A),type="l")
legend(1,8,legend=c("data","model"),col=c("blue","black"),lty=1:2,cex=0.8)
```

This looks pretty good.

## Part (c)
\fbox{\begin{minipage}{.8\textwidth}
Calculate the MSE.
Make a time plot, a ACF plot and a histogram for the residuals.
Does the residuals look like a normal white noise process?
\end{minipage}}

We compute the MSE in two different ways.
```{r}
df <- length(fit$residuals)-5
sse <- sum(fit$residuals^2)
mse <- sse/df
mse_alt <-summary(fit)$sigma^2  # agrees with mse calculation above
```
We see that the MSE is `r mse`.

The plot of the residuals is given by:
```{r}
plot(fit$residuals)
```

The residual plot is not especially demonstrative of white noise.
In particular, time units $40$ to $65$ seem to have a non-zero positive
expectation.
They should hover above and below more or less equally, but the expectation seems
to be roughly $0.5$ there.

The ACF of the residuals is given by:
```{r}
acf(fit$residuals)
```
According to the ACF, there seems to be some correlation.
Particularly, the periods seem to be correlated, i.e., lags $4$ and $8$ are
positively correlated, but it has positive correlations for other lags also.

The histogram of the residuals is given by:
```{r}
hist(fit$residuals)
```

By itself, this seems alright.
It has a mean of around $0$ and it is symmetric with a basic bell-shaped curve.

## Part (d)
\fbox{\begin{minipage}{.8\textwidth}
Make predictions for the first quarter in 1981.
Construct the $95\%$ prediction interval.
\end{minipage}}

```{r}
part_d_pred <- predict(fit,
                       newdata=data.frame(t=85,q1=1,q2=0,q3=0),
                       interval="prediction")
part_d_pred
```

Note that this is the prediction for the log-transformed data.
To compute the actual prediction, we must take the inverse of the transformation.

Let
$$
   A = \exp\left(\frac{1}{n}\sum_{i=1}^{n} \log y_i\right)
$$
then we transform the data with
$$
   y_i = A \log \rm{jj}_i,
$$
where $jj$ is the original data set (prior to the log transformation).
If we wish to undo the transformation, we do the inverse,
$$
   \hat{jj}_i = \exp\left(\frac{y_i}{A}\right).
$$

We apply this transformation to the predictions in the following R code:
```{r}
exp(part_d_pred/A)
```


## Part (e)
\fbox{\begin{minipage}{.8\textwidth}
Fit a additive model using the Holt-Winters method.
Let the function choose the optimal smoothing parameters automatically.
Report the smoothing parameters and coefficients.
Superimpose the fitted values on the time plot in part (a).
\end{minipage}}

First, we put the data into the format expected by $\operatorname{hw}$
by converting the frequency to $4$ ($4$ quarters per year), and then
use the Holt-Winters seasonal additive function to generate the model.
Finally, we report the model.
```{r warning=F, message=F}
library(forecast)
seasonal <- ts(data=ys, frequency=4)
holt_model <- hw(seasonal, level=c(95),h=1, seasonal="additive", initial="optimal")
summary(holt_model)
```

When we view this as a regression, we get, for instance
$\hat{\beta}_{0,T} = -1.5965$ and $\hat{\beta}_{1,T} = 0.117$.
The seasonal coefficients can be found in the $s$ initial state.

We plot the seasonal time series (now time is with respect to years, four
quarters per year) with the fitted model superimposed onto it with the
following R code:
```{r}
library(latex2exp)
plot(seasonal,col="blue",pch=19,xlab=TeX("year ($t$)"),ylab=TeX("$y_t$"))
lines(holt_model$fitted)
legend(1,8,legend=c("data","fitted"),col=c("blue","black"),lty=1:2,cex=0.8)
```

Now we show a plot of the untransformed fit.
```{r}
library(latex2exp)
plot(exp(seasonal/A),col="blue",pch=19,xlab=TeX("year ($t$)"),ylab=TeX("$jj_t$"))
lines(exp(holt_model$fitted/A))
```

## Part (f)
\fbox{\begin{minipage}{.8\textwidth}
Calculate the MSE for the Holt-Winters additive model and compare it with part (c).
Make a time plot, a ACF plot and a histogram for the residuals.
Does the residuals look like a normal white noise process?
\end{minipage}}

The MSE for the Holt-Winters model is given by
```{r}
round(holt_model$model$mse,digits=3)
```

In part (d), we had an MSE of `r round(mse,digits=3)`, which is larger than
the MSE for the Holt fit's MSE of `r round(holt_model$model$mse,digits=3)`.
This suggests a better fit was obtained by the Holt method.

We plot the residuals using the R code:
```{r}
plot(holt_model$residuals,type="b")
acf(holt_model$residuals)
hist(holt_model$residuals)
```

These residuals are compatible with white noise.
The ACF is compatible with lag times being uncorrelated,
the plot seems to jump up and down around the mean with constant
variance, and the histogram is more or less symmetric with a bell shape and
indicates a mean of $0$.

## Part (g)
\fbox{\begin{minipage}{.8\textwidth}
Make forecast using the Holt-Winters model for the first quarter in 1981.
Also report the $95\%$ prediction interval.
Compare the result with part (d).
\end{minipage}}

From the summary output in party (a), the forecast is `r round(holt_model$mean,digits=3)`
with a $95\%$ prediction interval
$$
   (`r round(holt_model$lower,digits=3)`,`r round(holt_model$upper,digits=3)`).
$$

By comparison, with the linear regression in part (d), we obtained the prediction
interval given by:
```{r}
   part_d_pred
```
The interval in part (d) is a bit larger (the prediction has more uncertainty),
but they are in reasonably close agreement.

The simpler smoothing model seems to model the data relatively well with
less uncertainty.
Needing to estimate the extra parameters in the regression model may be what
led to its larger variance.

We now show the prediction intervals with the log-transformation undone:
```{r}
exp(part_d_pred/A)
exp(c(holt_model$mean,holt_model$lower,holt_model$upper)/A)
```
# Problem 2.2
Suppose that $\{e_t\}$ is a zero mean white noise process with variance $\sigma^2$.
Let $\backshift$ denote the backshift operator.
Consider the processes:
\begin{enumerate}
\item[(i)] $(1 + 0.4 \backshift) Y_t = e_t$
\item[(ii)] $(1 − 0.9 \backshift)(1 − \backshift) Y_t = (1 − 0.5\backshift)(1 + 0.4\backshift) e_t$
\item[(iii)] $(1 − 0.4 \backshift − 0.45 \backshift^2) Y_t = (1 + \backshift + 0.25 \backshift^2) e_t$
\end{enumerate}

## Part (a)
\fbox{Identify each model as an $\operatorname{ARMA}(p, q)$ process; that is, specify $p$ and $q$.}

\begin{enumerate}
\item The process
$$
   (1 + 0.4 \backshift) Y_t = e_t
$$
has no redundancies in its representation.
We can determine the ARIMA model from this representation, but for clarity
we rewrite the process into its ``canonical'' form,
$$
   Y_t = -0.4 Y_{t-1} + e_t.
$$

We see that it is a zero mean $\operatorname{ARMA}(p=1, q=0) \equiv \operatorname{AR}(1)$
process.

\item The process
$$
   (1 − 0.9 \backshift)(1 − \backshift) Y_t =
      (1 − 0.5\backshift)(1 + 0.4\backshift) e_t.
$$
has no redundancies in its representation.
We can determine the ARIMA model from this representation, but for clarity
we seek its ``canonical'' form by expanding the factored form into
$$
   (1 - 1.9 \backshift + 0.9 \backshift^2) Y_t =
      (1 - 0.1 \backshift - 0.2 \backshift^2) e_t.
$$
and then rewriting as
$$
   Y_t = 1.9 Y_{t-1} - 0.9 Y_{t-2} + e_t - 0.1 e_{t-1} - 0.2 e_{t-2},
$$
which models a zero mean $\operatorname{ARIMA}(p=2,q=2)$ process.

\item The process
$$
   (1 − 0.4 \backshift − 0.45 \backshift^2) Y_t =
      (1 + \backshift + 0.25 \backshift^2) e_t.
$$
can be factored as
$$
   (1 − 0.9 \backshift)(1 + 0.5 \backshift) Y_t =
      (1 + 0.5 \backshift)(1 + 0.5 \backshift) e_t,
$$
which shows that the representation has redundancy.
We may remove this redundancy by dividing both sides by the common factor
$1 + 0.5 \backshift$, resulting in the equivalent expression
$$
   (1 − 0.9 \backshift) Y_t = (1 + 0.5 \backshift) e_t.
$$

We can determine the ARIMA model from this representation, but for clarity
we rewrite the process into its ``canonical'' form,
$$
   Y_t = 0.9 Y_{t-1} + e_t + 0.5 e_{t-1},
$$
which models a zero mean $\operatorname{ARIMA}(p=1,q=1)$ process.
\end{enumerate}

## Part (b)
\fbox{\begin{minipage}{.8\textwidth}
Give the autocorrelation function $ρ_k$ for those processes which are stationary.
If you want, you can use the $\operatorname{ARMAacf}$ function in R to see the
first dozen or so correlations.
\end{minipage}}

### Process (i)
Process $Y_t = -0.4 Y_{t-1} + e_t$ is stationary since $|-0.4| < 1$.
Its autocorrelation function is
$$
   ρ_k = (-0.4)^k.
$$

We output the first dozen lags using the following R code:
```{r}
p1 <- ARMAacf(ar=c(-.4),lag.max=12,pacf=F)
p1
```

### Process (ii)
Process $Y_t = 1.9 Y_{t-1} - 0.9 Y_{t-2} + e_t - 0.1 e_{t-1} - 0.2 e_{t-2}$
has the more convenient representation
$$
   (1 − 0.9 \backshift)(1 − \backshift) Y_t =
      (1 − 0.5\backshift)(1 + 0.4\backshift) e_t.
$$
for determining stationarity condition.
We let $φ(x) = (1 − 0.9 x)(1 − x)$ and see that the roots of $φ$ are
$\{1, 10/9\}$.
Since one of the roots is not greater than $1$, the process
is non-stationary.

### Process (iii)
Process $Y_t = 0.9 Y_{t-1} + e_t + 0.5 e_{t-1}$ has the more convenient
representation
$$
   (1 − 0.9 \backshift) Y_t = (1 + 0.5 \backshift) e_t.
$$
for determining stationary condition.
We let $φ(x) = (1 − 0.9 x)$ and see that the root of $φ$ is $10/9$, which
is greater than $1$ and therefore the process is stationary.

The autocorrelation function is given by
$$
   ρ_k = 0.6984(0.9)^k
$$
for $k > 0$.
\begin{proof}
For an $\operatorname{ARIMA}(1,1)$ model, its form is given by
$$
   Y_t = φ Y_{t-1} + e_t - \theta e_{t-1}.
$$
We match this to $Y_t = 0.9 Y_{t-1} + e_t - (-0.5) e_{t-1}$ and see that
$φ = 0.9$ and $\theta = -0.5$.

Note that
\begin{align*}
   γ_0 &= \sigma^2\left(1 + \frac{(φ + \theta)^2}{1 - φ^2}\right)\\
       &= \sigma^2\left(1 + \frac{(0.9 - 0.5)^2}{1 - 0.9^2}\right)\\
       &= 1.8421 \sigma^2
\end{align*}
and
\begin{align*}
   γ_1 &= \sigma^2\left(φ + \theta + φ\frac{(φ + \theta)^2}{1 - φ^2}\right)\\
       &= 1.1579 \sigma^2
\end{align*}

Then, $ρ_1 = γ_0 / γ_1 = 0.6286$ and
$$
   ρ_k = φ^{k-1} ρ_1 = 0.6984(0.9)^k
$$
for $k > 0$.
\end{proof}

We output the first dozen lags using the following R code:
```{r}
ARMAacf(ar=c(.9),ma=c(-.5),lag.max=12,pacf=F)
```

## Part (c)
\fbox{\begin{minipage}{.8\textwidth}
Simulate a data set from each process identified above show the data set in a
time series plot and also show the sample ACF for it.
In each case, does the time plot agree with the stationarity?
Does the sample ACF agree with what we know to be true from the theory?
You may pick your favorite sample size (anything larger than $100$) and the
white noise variance.
\end{minipage}}

### Process (i)
The process defined as
$$
   Y_t = -0.4 Y_{t-1} + e_t.
$$
is an $\operatorname{AR}(1)$ model.
We simulate drawing a sample and plotting the sample ACF with the following R code:
```{r}
sigma <- 1
ts1 <- arima.sim(n = 1000,
          list(ar = c(-0.4)),
          sd = sigma)
acf(ts1)
pacf(ts1)
```

We see that the sample ACF of the $\operatorname{AR}(1)$ process has the following
characteristics:
\begin{enumerate}
\item Oscillates as expected since it has a negative coefficient $-0.4$. 
\item Seems to exponentially decay, as expected of an AR model.
\end{enumerate}

More importantly, the sample PACF is compatible with a non-zero correlation
only at lag $1$, as expected.

### Process (iii)
The process defined as
$$
   Y_t = 0.9Y_{t-1} + e_t - (-.5)e_{t-1}.
$$
is an $\operatorname{ARMA}(1,1)$ model.
We simulate drawing a sample and plotting the sample ACF with the following R code:
```{r}
ts2 <- arima.sim(n = 1000,
                 list(ar = c(0.9),
                      ma = c(-0.5)),
                 sd = sigma)
acf(ts2)
pacf(ts2)
```

We see that the sample ACF of $\operatorname{ARMA}(1,1)$ process has the following
characteristics:
\begin{enumerate}
\item ACF has no cut-off. The observed correlation is compatible with being
non-zero for many lags, as expected for a time series with an autoregressive component.
This looks like an $\operatorname{AR}$ process.
\item Both the ACF and PACF exponentially decays (with a damped sinusoidal component), as expected.
It is a mixture of $\operatorname{AR}$ and $\operatorname{MA}$.
\end{enumerate}
