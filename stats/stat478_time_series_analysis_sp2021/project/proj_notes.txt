(1) From multiple time series with optimal adversaries, parameterize
the confidentiality statistic on the entropy of the plaintext, i.e., an optimal adversary is estimated to produce a confidentiality statistic 
given by:

    confidentiality(t | H)

where H is entropy measure.

I don't know if that'd pan out, but if it can be estimated or bounded
then that might be interesting. Since we have an optimal adversary,
the sophistication of the true plaintext distribution could be trivial
and maybe we can either argue that this is a pessimistic lower-bound on
confidentiality for any other system, or we can conjecture or prove
some result about it.

I would also like to extend to more sophisticated encrypted search
systems that do not use a naive simple substitution cipher. There are
many tricks that can be done to make the task for the adversary
much harder without too much effort, but I'll have to look more
closely at existing literature.

When I have some time, and you are interested, I can give you some
of my preliminary findings / research.

(2) Instead of using a scalar confidentiality statistic to perform the estimation of the confidentiality, do a logistic regression where we estimate the probability that some plaintext x[i] has been decoded with respect to time and an additional set of predictor variables, such as the rank of x[i] (where rank is with respect to marginal probability, i.e., if P[x1]=.2, P[x2]=.3, and P[x3]=.5, then x1 has rank 3, x2 has rank 2, and
x3 has rank 1). I believe we could use a logistic regression to
learn 



########################


accuracy X; support [0,1]

E[X_0] is near 0
lim t -> infinity E[X_t] = c

X is the proportion of time series {c_t}
understood by adversary.

What is the probability that c_T
will be understood? If we do not
consider what came before, it is
X_T since at time T adversary
understands that proportion.

If X_T = x_t then adversary
understands c_T with probability
x_t.

If x_T is not given, and 
x_1,...,x_{T-1} is not given,
then adversary understands
c_T with probability E[X_T].
This may be hard to compute.

If x_T is not given, but
x_1,...,x_{T-1} is given,
then we can do a time series
forecast on x_T.

Let's denote x_T by pi(T)
so make it explicit that we're
measuring a probability.

We can use a CDF since it has
the desired shape, except that
pi has an expectation that
ranges between 0 and 0 < c <= 1.
We scale the CDF.

pi(t) = 1-exp(-B t)
pi(0) = 1-1=0
pi(inf) = 1-0 = 1


pi(t) = 1 - A exp(-B t)
pi(0) = 1 - A*1 = 1-A
pi(inf) = 1 - A*0 = 1

---
pi(t) = C - A exp(-B t)

pi(0) = C - A
pi(inf) = C

So, pi(t) is an expectation, and it
ranges between C-A and C. If A = C - alpha,
then pi(t) is an expectation that
ranges between alpha and C.

pi(t) = a+bexp(c*t)
ys <- I(a) - I(b) * exp(I(c)*ts)

df <- data.frame(ts,ys)
nls(ys ~ I(c) - I(b)*exp(I(a)*xs), data = df, start = list(b=1,a=-1,c=.1), trace = T)

we can fit!


exp.eq <- function(x, a, b) {
    exp(1)^(a + b * sin(x^4))
}
exp.eq(2, 1, 3)  # testing the equation works: 3^2 + 1 = 10

m.sinexp <- nls(y ~ exp.eq(x, a, b), data = df, start = list(a = 1, b = 1), trace = T)

plot(x, y)
lines(s, s^3, lty = 2)
lines(s, predict(m, list(x = s)), col = "green")
lines(s, predict(m.exp, list(x = s)), col = "red")
lines(s, predict(m.sinexp, list(x = s)), col = "blue")



A monotone regression curve has the shape of a cumulative
distribution function (cdf) for a continuous random variable.
This suggests a model for response pi(t) having form pi(t) = F(t) for some cdf F.

Except, we must modify F as described above.


instead of pi(t) = F(t), we can also consider
pi(t) = F(a*t + b2*x1 + d*x2 +


a e^(t + c) = e^a' e^(t+c) = e^(t+c+a)

pi(t) = a - b exp(c t)

we can extend this:
pi(x1,x2,x3) = a - exp(b0 + b1 x1 + b2 x2 + b3 x3 + b4 x3 x4)
where x1 = t.

now we can fit all kinds of things to it.




back to pi(t). we have noise, so is it pi(t) + n_t, which
means it can go above 1 and below 0! not what we want, but
maybe okay.

another way to look at it: pi'(t) = pi(t + Q * n_t)
so we have another thing to estimate, but now we
see that it follows the curve as before, but shifts
back and forth on it. this isn't what we want either!

do we need to introduce more parameters?




F(0) = 0
F(inf) = 1

pi(t) = b0 + b1 F(b2 t + b3 x1(t) + b4 x2(t) + b5 x1(t) x2(t)) + n(t)
where n(t) ~ arima(p,d,q)

pi(0) = b0 + b1 F(b3 x1(0) + b4 x2(0) + b5 x1(0) x2(0)) + n(0)
suppose xj(0) = 0 and n(0) = 0
pi(0) = b0


pi(inf) = b0 + b1 F(b2 inf + b3 x1(inf) + b4 x2(inf) + b5 x1(inf) x2(inf)) + n(inf)
suppose xj(inf) < inf and b2 > 0, then
pi(inf) = b0 + b1 + n(inf)
note that 0 < b0 + b1 < 1
so, the expectation of pi(t) starts at b0 and ends up at b0 <= b0+b1 < 1
if b1 = 0, then pi is constant + noise.
